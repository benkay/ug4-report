\chapter{Formulation of the Problem \& Proposed Solution}

The main obstacles to effective usability testing on the Android platform are
cost, expertise required and the lack of effective automation.  If performing
think aloud testing, participants must be found and then travel to the site of
the testing (or the developer travel to them), and are usually paid for their
time. While there are services that take care of much of the process on behalf
of the developer, the process still tends to be costly, especially if it is done
often.  Other types of usability testing such as heuristic evaluation must be
performed by experts to be effective.  In any case,
performing just one from of usability testing is likely to leave some issues
unfound.

Other types of testing are usually tightly integrated into the software development
cycle, with feedback available immediately and continuously throughout the process.
These obstacles, however, prevent usability testing being treated in the same manner,
and may not be performed often or
even at all during the development an application. The relative immaturity of the mobile
ecosystem makes matters worse, with many of the tools and experience from web and
desktop development unavailable to Android developers. The result is usability
issues in the user interface that go undetected by the point an application is
released to users, and some that remain long after that.

\section{Background to Solution}

\subsection{Crowdsourcing}

Crowdsourcing has shown to be an effective way of performing repetitive tasks
which aren't suitable for computers - i.e.\ which need to be done by a human. It
works by getting many people to perform a small amount of work, the results of
which can be combined to form something useful. Perhaps some of the best known
examples of applications employing crowdsourcing were created by Luis Von Ahn
at Carnegie Mellon University, and include the \emph{ESP Game}, where
participants collaborate over the internet to assign machine readable tags to
images and help search results, \emph{reCAPTCHA}, which leverages the popular
``captcha'' method of preventing automated bots from completing web forms to
digitise books, and most recently \emph{Duolingo}, a service which aims to
teach people new languages while translating web pages.

%Modern software development is moving to a more iterative process with
%a ``release early, release often'' attitude. This trend is particularly visible
%in the mobile space, where the major platforms' app stores provide an
%integrated and efficient delivery mechanism for updates. Combined with modern
%analytics packages and other user feedback mechanisms such as support emails
%and reviews this gives developers an opportunity to test their applications'
%interface and gradually refine the user experience after release.
%
%Indeed when using analytics in an application this already seems like a form of
%crowdsourcing - the entire userbase of the application is potentially involved
%in the application's improvement. 

\subsection{Gamification}

In the crowdsourcing examples given earlier, the ESP game and Duolingo have one
thing in common - a way to keep people engaged so they continue using the
application. The latter one does this by providing a useful service to the
user, and the former by turning the experience into a game.

The process of integrating game oriented features into a non-gaming environment
is known as Gamification, a concept that took off in mainstream computing
around 2010 \cite{gamification-trends}. It commonly involves adding rewards
such as points or other forms of recognition for completing tasks or challenges
within the environment.

The gamification gives the user an incentive to continue using a service or
piece of software, turning any work they may be doing into a form of 
entertainment. For this reason it is commonly used in conjunction with
crowdsourcing, keeping users as participants long after they might otherwise
have quit.

\subsection{Analytics}

Analytics services have been available since the early days of the web,
tracking user's visits and their behaviour on web sites. As the web evolved and
commercial sites appeared, analysis of analytics data became useful in
identifying problem areas in the user experience. Some academic work in this
area has been done, for example Hasan et al \cite{hasan2009using} explore the
use of metrics in Google Analytics to evaluate the usability of e-commerce
sites, finding it to be an effective tool for quickly and easily identifying
some problem areas.

Analytics are also widely used in mobile apps.
These services typically measure specifics about the user-base of
the application, yielding useful data such as user count, frequency of use and
user retention, as well as estimates of their geographic distribution and
average age/gender. Most services also allow the developer to measure feature
usage by integrating predefined ``events'', which get triggered by a user when they perform
an action in the application. For example in a music app, the developer
might add an event for the play button. Whenever the user then presses play, an
event is sent to the analytics service and the developer can view statistics
showing, say, the number of people who pressed play in a single day or the
proportion of total users that use that feature. By incorporating these events
throughout the application, the developer can get an idea of which features are
heavily used, and what functionality is exerted more rarely. The obvious
limitation of this approach is that it is difficult to discern the intent of
the user; in most scenarios, the developer cannot distinguish between
functionality that is not used because users have no desire to use it, and
functionality that is not used because it is not obvious, or has poor
usability. The exceptions here are use cases that where a specific workflow is
involved, for example a signup form or purchase. In this case, events can
be placed at each point of the process and the dropout rate at each stage can
be measured. If a particular stage of the process has a high dropout rate, then
that stage can be evaluated for usability problems.

\section{Proposed Solution}

The idea behind the project is to leverage crowdsourcing and related techniques
provide a new option for testing the interface of an Android app. While it
would be unlikely to (and the project does not aim to) completely replace
normal user testing, a cheaper and quicker method could be performed more often
(or even continuously), driving down the cost of producing a more polished app.


\subsection{Initial Design}

Early designs and implementations focused on the gamification of the application
being tested. The game encouraged the user to gather ``eggs'', or ``achievements''
by navigating to areas of the interface and using features, the idea being that
the eggs that few people managed to gather may reside in areas with poor usability.
If the user could see which achievements had been gathered, or how many eggs
they had collected out of the total, they would be encouraged to continue the
game, furthering the testing.
Since the project also aimed to gather other useful data about the specific
interactions which were problematic, it became clear that linking navigation
data to the collection of the eggs would be difficult due to similar reasons
to analytics, where the intent behind each user decision is not available.
The focus of the project moved to investigating ways of effectively gathering
this information, and although the tasks in the end result closely resemble
the achievements used initially, the project moved to a scenario where the
participant would complete the testing in a single short sitting, as opposed
to a lengthy game where the user needs an incentive to stay engaged.

\subsection{Focus On Data}

While analytics services can gather large amounts of data from the interaction
of users with an application, the lack of structure can make it difficult to
draw meaningful conclusions as information about the intent of the user is lost
when the data is gathered.  Without knowing this information, it is hard to
discern areas of the interface with poor usability versus areas which are
simply less popular.

As mentioned, tasks which always have a specific structured workflow, such as a checkout
process, are easier to analyse with existing analytics services, as user intent
is clear. This is similar to the use of specific tasks given to the participant
in user testing. If participants are given specific tasks to perform, and their
interaction with the app logged in a manner similar to analytics, the same
usefulness of analytics can be extended to other areas of the app.

By combining some of the methodologies traditionally used in usability testing
with analytics and crowdsourcing, we should be able to provide an environment
that can be given to people unfamiliar with an application, and easily
gather data which will identify potential problem areas with respect to
usability.

The goal of this project is to produce a library that can be quickly integrated into
any Android app, with as little modification to the application's code
as possible. The library should enable users to help test the interface
of the application without any intervention or need for the developer to
be present. It should also gather data and present it to the developer,
with the aim of highlighting good and poor usability areas in the user interface
so they can be easily identified and investigated further.

To be successful, the library should be easy and quick to integrate, and
provide results that can be easily interpreted to achieve the above goals.
